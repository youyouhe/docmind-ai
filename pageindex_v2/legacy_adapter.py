"""
Legacy Adapter for PageIndex V2
å…¼å®¹è€ pageindex API çš„é€‚é…å±‚

æä¾›ä¸è€ç‰ˆæœ¬ page_index_main å®Œå…¨ç›¸åŒçš„æ¥å£ï¼Œç¡®ä¿ä¸‹æ¸¸ç³»ç»Ÿæ— éœ€ä¿®æ”¹å³å¯ä½¿ç”¨æ–°ç®—æ³•ã€‚

Usage:
    from pageindex_v2.legacy_adapter import page_index_main, config, ConfigLoader
    
    opt = ConfigLoader().load({"model": "gpt-4o-2024-11-20"})
    result = page_index_main("path/to/file.pdf", opt)
"""

import os
import asyncio
import time
from io import BytesIO
from typing import Dict, Any, Optional, Union
from types import SimpleNamespace


def config(**kwargs):
    """
    å…¼å®¹è€ç‰ˆæœ¬çš„ config å¯¹è±¡ï¼ˆSimpleNamespaceï¼‰
    """
    return SimpleNamespace(**kwargs)


class ConfigLoader:
    """
    å…¼å®¹è€ç‰ˆæœ¬çš„ ConfigLoader
    åŠ è½½é»˜è®¤é…ç½®å¹¶ä¸ç”¨æˆ·é…ç½®åˆå¹¶
    """
    
    DEFAULT_CONFIG = {
        "model": "gpt-4o-2024-11-20",
        "toc_check_page_num": 20,
        "max_page_num_each_node": 10,
        "max_token_num_each_node": 20000,
        "if_add_node_id": "yes",
        "if_add_node_summary": "no",
        "if_add_doc_description": "no",
        "if_add_node_text": "no",
        "custom_prompt": None
    }
    
    def load(self, user_opt: Optional[Union[Dict, SimpleNamespace]] = None) -> SimpleNamespace:
        """
        åŠ è½½é…ç½®ï¼Œåˆå¹¶ç”¨æˆ·é€‰é¡¹ä¸é»˜è®¤å€¼
        
        Args:
            user_opt: ç”¨æˆ·é…ç½®ï¼ˆdict æˆ– SimpleNamespaceï¼‰
            
        Returns:
            SimpleNamespace é…ç½®å¯¹è±¡
        """
        if user_opt is None:
            user_dict = {}
        elif isinstance(user_opt, SimpleNamespace):
            user_dict = vars(user_opt)
        elif isinstance(user_opt, dict):
            user_dict = user_opt
        else:
            raise TypeError("user_opt must be dict, SimpleNamespace or None")
        
        # åˆå¹¶é…ç½®
        merged = {**self.DEFAULT_CONFIG, **user_dict}
        return config(**merged)


def page_index_main(doc: Union[str, BytesIO], opt: Optional[SimpleNamespace] = None) -> Dict[str, Any]:
    """
    ä¸»å…¥å£å‡½æ•° - å…¼å®¹è€ç‰ˆæœ¬ pageindex.page_index_main API
    
    Args:
        doc: PDFæ–‡ä»¶è·¯å¾„ï¼ˆstrï¼‰æˆ– BytesIO å¯¹è±¡
        opt: é…ç½®å¯¹è±¡ï¼ˆç”± ConfigLoader ç”Ÿæˆï¼‰
        
    Returns:
        å…¼å®¹è€æ ¼å¼çš„è¾“å‡ºï¼š
        {
          "result": {
            "doc_name": "xxx.pdf",
            "structure": [...]
          },
          "performance": {...}
        }
    """
    # åŠ è½½é»˜è®¤é…ç½®
    if opt is None:
        opt = ConfigLoader().load()
    
    # è½¬æ¢é…ç½®åˆ° ProcessingOptions
    options = _convert_old_opt_to_v2(opt)
    
    # å¤„ç† BytesIO è¾“å…¥
    pdf_path, temp_file = _prepare_pdf_input(doc)
    
    try:
        # å¯¼å…¥æ–°ç®—æ³•ï¼ˆä½¿ç”¨ç›¸å¯¹å¯¼å…¥ï¼‰
        from .main import PageIndexV2
        
        # è®¾ç½® document_idï¼ˆç”¨äº progress callbackï¼‰
        _setup_progress_callback()
        
        # è°ƒç”¨æ–°ç®—æ³•
        start_time = time.time()
        processor = PageIndexV2(options)
        
        # åŒ…è£…è¿›åº¦æŠ¥å‘Š
        _wrap_progress_reporting(processor)
        
        # æ‰§è¡Œå¤„ç†ï¼ˆå¼‚æ­¥è½¬åŒæ­¥ï¼‰
        v2_result = asyncio.run(processor.process_pdf(pdf_path))
        
        total_time = time.time() - start_time
        
        # è½¬æ¢è¾“å‡ºæ ¼å¼
        old_format = _convert_v2_to_old_format(v2_result, opt, total_time)
        
        # åå¤„ç†ï¼šæ·»åŠ  node_id, text, summaryï¼ˆæ ¹æ®é…ç½®ï¼‰
        structure = old_format["result"]["structure"]
        
        if getattr(opt, 'if_add_node_id', 'yes') == 'yes':
            _add_node_ids(structure)
        
        if getattr(opt, 'if_add_node_text', 'no') == 'yes':
            _add_node_text(structure, pdf_path)
        
        if getattr(opt, 'if_add_node_summary', 'no') == 'yes':
            # Summary éœ€è¦ textï¼Œå¦‚æœä¹‹å‰æ²¡åŠ ï¼Œä¸´æ—¶åŠ ä¸Š
            needs_temp_text = getattr(opt, 'if_add_node_text', 'no') == 'no'
            if needs_temp_text:
                _add_node_text(structure, pdf_path)
            
            # ç”Ÿæˆ summariesï¼ˆå¼‚æ­¥ï¼‰
            asyncio.run(_add_node_summaries(structure, getattr(opt, 'model', 'gpt-4o-2024-11-20')))
            
            # ç§»é™¤ä¸´æ—¶ text
            if needs_temp_text:
                _remove_node_text(structure)
        
        return old_format
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_file:
            try:
                os.unlink(temp_file)
            except:
                pass


def _convert_old_opt_to_v2(opt: SimpleNamespace):
    """
    å°†è€é…ç½®è½¬æ¢ä¸ºæ–°çš„ ProcessingOptions
    
    æ˜ å°„å…³ç³»ï¼š
    - model: ç›´æ¥æ˜ å°„
    - toc_check_page_num -> toc_check_pages
    - max_page_num_each_node -> max_pages_per_node
    - max_token_num_each_node -> max_tokens_per_node
    """
    from .main import ProcessingOptions
    
    # ç¡®å®š providerï¼ˆæ ¹æ® model æ¨æ–­ï¼‰
    model = getattr(opt, 'model', 'gpt-4o-2024-11-20')
    if 'deepseek' in model.lower():
        provider = 'deepseek'
    else:
        provider = 'openai'
    
    return ProcessingOptions(
        provider=provider,
        model=model,
        max_depth=4,  # æ–°ç®—æ³•å›ºå®šä¸º4å±‚
        toc_check_pages=getattr(opt, 'toc_check_page_num', 20),
        debug=False,  # å…³é—­è°ƒè¯•è¾“å‡ºï¼Œé¿å…å¹²æ‰°è€ç³»ç»Ÿæ—¥å¿—
        progress=True,  # ä¿æŒè¿›åº¦è¾“å‡º
        output_dir="./results",
        enable_recursive_processing=True,
        skip_verification_for_large_pdf=True,
        large_pdf_threshold=200,
        max_pages_per_node=getattr(opt, 'max_page_num_each_node', 10),
        max_tokens_per_node=getattr(opt, 'max_token_num_each_node', 20000),
        max_verify_count=100,
        verification_concurrency=20
    )


def _prepare_pdf_input(doc: Union[str, BytesIO]) -> tuple:
    """
    å¤„ç† PDF è¾“å…¥ï¼Œæ”¯æŒæ–‡ä»¶è·¯å¾„å’Œ BytesIO
    
    Returns:
        (pdf_path, temp_file_path)
        - pdf_path: å®é™…çš„æ–‡ä»¶è·¯å¾„
        - temp_file_path: å¦‚æœæ˜¯ BytesIOï¼Œè¿”å›ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼›å¦åˆ™è¿”å› None
    """
    if isinstance(doc, BytesIO):
        # BytesIO éœ€è¦ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        import tempfile
        
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf', mode='wb') as tmp:
            tmp.write(doc.getvalue())
            temp_path = tmp.name
        
        return temp_path, temp_path
    
    elif isinstance(doc, str):
        # å­—ç¬¦ä¸²è·¯å¾„
        if not os.path.isfile(doc):
            raise FileNotFoundError(f"PDF file not found: {doc}")
        return doc, None
    
    else:
        raise TypeError(f"Unsupported input type: {type(doc)}. Expected str or BytesIO.")


def _setup_progress_callback():
    """
    è®¾ç½®è¿›åº¦å›è°ƒï¼ˆå…¼å®¹è€ç³»ç»Ÿçš„ progress_callback æ¨¡å—ï¼‰
    """
    try:
        from pageindex.progress_callback import get_document_id
        doc_id = get_document_id()
        # document_id å·²é€šè¿‡ pageindex.progress_callback è®¾ç½®
        return doc_id
    except ImportError:
        # progress_callback æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡
        return None


def _wrap_progress_reporting(processor):
    """
    åŒ…è£… PageIndexV2 çš„è¿›åº¦æŠ¥å‘Šï¼Œè½¬å‘åˆ°è€ç³»ç»Ÿçš„ progress_callback
    
    Args:
        processor: PageIndexV2 å®ä¾‹
    """
    try:
        from pageindex.progress_callback import report_progress, get_document_id
        
        doc_id = get_document_id()
        if not doc_id:
            return  # æ²¡æœ‰ document_idï¼Œè·³è¿‡
        
        # ä¿å­˜åŸå§‹å‡½æ•°
        original_log_progress = processor.log_progress
        
        def wrapped_log_progress(message: str, force: bool = False):
            """åŒ…è£…åçš„è¿›åº¦æŠ¥å‘Šå‡½æ•°"""
            # è°ƒç”¨åŸå§‹å‡½æ•°
            original_log_progress(message, force)
            
            # è§£ææ¶ˆæ¯æå–é˜¶æ®µå’Œè¿›åº¦
            stage, progress_pct, clean_msg = _parse_progress_message(message)
            
            if stage and progress_pct is not None:
                # è½¬å‘åˆ°è€ç³»ç»Ÿ
                report_progress(doc_id, stage, progress=progress_pct, message=clean_msg)
        
        # æ›¿æ¢å‡½æ•°
        processor.log_progress = wrapped_log_progress
        
    except ImportError:
        # progress_callback ä¸å¯ç”¨ï¼Œè·³è¿‡
        pass


def _parse_progress_message(message: str) -> tuple:
    """
    è§£æè¿›åº¦æ¶ˆæ¯ï¼Œæå–é˜¶æ®µåå’Œè¿›åº¦ç™¾åˆ†æ¯”
    
    Args:
        message: å¦‚ "ğŸ“„ [1/6] PDF Parsing... (30 pages)"
        
    Returns:
        (stage, progress_percent, clean_message)
    """
    import re
    
    # é»˜è®¤å€¼
    stage = "processing"
    progress_pct = None
    clean_msg = message
    
    # åŒ¹é…æ¨¡å¼: [X/Y]
    match = re.search(r'\[(\d+)/(\d+)\]', message)
    if match:
        current = int(match.group(1))
        total = int(match.group(2))
        # æ˜ å°„åˆ° 0-100 çš„è¿›åº¦
        progress_pct = (current - 1) / total * 100
        
        # æå–é˜¶æ®µå
        stage_match = re.search(r'\[(\d+)/\d+\]\s*([^.]+)', message)
        if stage_match:
            stage_name = stage_match.group(2).strip()
            # æ˜ å°„åˆ°è€ç³»ç»Ÿçš„é˜¶æ®µå
            stage_mapping = {
                'PDF Parsing': 'pdf_parsing',
                'TOC Detection': 'toc_detection',
                'Structure Extraction': 'toc_transformation',
                'Page Mapping': 'page_mapping',
                'Verification': 'toc_verification',
                'Tree Building': 'tree_building'
            }
            stage = stage_mapping.get(stage_name, 'processing')
    
    return stage, progress_pct, clean_msg


def _convert_v2_to_old_format(v2_result: Dict, opt: SimpleNamespace, total_time: float) -> Dict[str, Any]:
    """
    è½¬æ¢æ–°ç®—æ³•è¾“å‡ºä¸ºè€æ ¼å¼
    
    Args:
        v2_result: æ–°ç®—æ³•çš„è¾“å‡º
        opt: é…ç½®å¯¹è±¡
        total_time: æ€»å¤„ç†æ—¶é—´
        
    Returns:
        è€æ ¼å¼è¾“å‡ºï¼š
        {
          "result": {
            "doc_name": "...",
            "structure": [...]
          },
          "performance": {...}
        }
    """
    # æå–åŸºæœ¬ä¿¡æ¯
    doc_name = v2_result.get("source_file", "unknown.pdf")
    structure = v2_result.get("structure", [])
    stats = v2_result.get("statistics", {})
    
    # æ ¹æ®é…ç½®æ·»åŠ  doc_descriptionï¼ˆå¦‚æœéœ€è¦ï¼‰
    result_dict = {
        "doc_name": doc_name,
        "structure": structure
    }
    
    if getattr(opt, 'if_add_doc_description', 'no') == 'yes' and v2_result.get("doc_description"):
        result_dict["doc_description"] = v2_result.get("doc_description")
    
    # æ„é€ æ€§èƒ½æ•°æ®ï¼ˆå…¼å®¹è€æ ¼å¼ï¼‰
    performance = {
        "total_time": total_time,
        "tree_building": {
            "duration": total_time * 0.7,  # ä¼°ç®—ï¼šæ ‘æ„å»ºå 70%
            "items_processed": stats.get("total_nodes", 0)
        },
        "toc_detection": {
            "duration": total_time * 0.1,  # ä¼°ç®—ï¼šTOCæ£€æµ‹å 10%
        },
        "toc_transformation": {
            "duration": total_time * 0.1,  # ä¼°ç®—ï¼šè½¬æ¢å 10%
        },
        "verification": {
            "duration": total_time * 0.1,  # ä¼°ç®—ï¼šéªŒè¯å 10%
            "accuracy": v2_result.get("verification_accuracy", 1.0)
        },
        "summary": {
            "total_nodes": stats.get("total_nodes", 0),
            "max_depth": stats.get("max_depth", 0),
            "root_nodes": stats.get("root_nodes", 0),
            "mapping_accuracy": v2_result.get("mapping_validation_accuracy", 1.0),
            "verification_accuracy": v2_result.get("verification_accuracy", 1.0)
        }
    }
    
    # è¿”å›å…¼å®¹æ ¼å¼
    return {
        "result": result_dict,
        "performance": performance
    }


# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæ·»åŠ  node_id, text, summary
# ============================================================================

def _add_node_ids(structure: list, node_id: int = 0):
    """
    é€’å½’æ·»åŠ  node_idï¼ˆæ ¼å¼ï¼š0000, 0001, 0002...ï¼‰
    
    Args:
        structure: æ ‘ç»“æ„ï¼ˆåˆ—è¡¨ï¼‰
        node_id: å½“å‰èŠ‚ç‚¹IDè®¡æ•°å™¨
    """
    for item in structure:
        item['node_id'] = str(node_id).zfill(4)
        node_id += 1
        
        if 'nodes' in item and item['nodes']:
            node_id = _add_node_ids(item['nodes'], node_id)
    
    return node_id


def _add_node_text(structure: list, pdf_path: str):
    """
    æ·»åŠ èŠ‚ç‚¹æ–‡æœ¬å†…å®¹
    
    ç­–ç•¥ï¼š
    - å¶å­èŠ‚ç‚¹ï¼šæ·»åŠ æˆªæ–­çš„å†…å®¹ï¼ˆå‰500å­—ç¬¦ï¼‰
    - çˆ¶èŠ‚ç‚¹ï¼šä¸æ·»åŠ å†…å®¹ï¼ˆä½¿ç”¨ summary æ›¿ä»£ï¼‰
    
    Args:
        structure: æ ‘ç»“æ„
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
    """
    import fitz  # PyMuPDF
    
    # æ‰“å¼€PDF
    doc = fitz.open(pdf_path)
    
    def extract_text_from_pages(start: int, end: int) -> str:
        """æå–æŒ‡å®šé¡µé¢èŒƒå›´çš„æ–‡æœ¬"""
        text_parts = []
        for page_num in range(start - 1, min(end, len(doc))):
            if page_num >= 0:
                page = doc[page_num]
                text_parts.append(page.get_text())
        return "\n".join(text_parts)
    
    def add_text_recursive(node):
        """é€’å½’æ·»åŠ æ–‡æœ¬"""
        has_children = 'nodes' in node and node['nodes']
        
        if not has_children:
            # å¶å­èŠ‚ç‚¹ï¼šæ·»åŠ æ–‡æœ¬
            start = node.get('start_index', 1)
            end = node.get('end_index', 1)
            full_text = extract_text_from_pages(start, end)
            # æˆªæ–­åˆ°500å­—ç¬¦
            node['text'] = full_text[:500] if len(full_text) > 500 else full_text
        else:
            # çˆ¶èŠ‚ç‚¹ï¼šç©ºæ–‡æœ¬
            node['text'] = ""
            
            # é€’å½’å¤„ç†å­èŠ‚ç‚¹
            for child in node['nodes']:
                add_text_recursive(child)
    
    # å¤„ç†æ‰€æœ‰æ ¹èŠ‚ç‚¹
    for root in structure:
        add_text_recursive(root)
    
    doc.close()


async def _add_node_summaries(structure: list, model: str):
    """
    å¼‚æ­¥ç”ŸæˆèŠ‚ç‚¹æ‘˜è¦
    
    Args:
        structure: æ ‘ç»“æ„
        model: LLMæ¨¡å‹åç§°
    """
    from .core.llm_client import LLMClient
    
    # åˆå§‹åŒ– LLM client
    if 'deepseek' in model.lower():
        provider = 'deepseek'
    else:
        provider = 'openai'
    
    llm = LLMClient(provider=provider, model=model, debug=False)
    
    async def generate_summary(node):
        """ä¸ºå•ä¸ªèŠ‚ç‚¹ç”Ÿæˆæ‘˜è¦"""
        text = node.get('text', '')
        title = node.get('title', '')
        
        if not text or len(text.strip()) < 10:
            node['summary'] = ""
            return
        
        # æˆªæ–­æ–‡æœ¬ï¼ˆé¿å…tokenè¶…é™ï¼‰
        truncated_text = text[:3000]
        
        prompt = f"""Summarize the following section from a document in 1-2 sentences.

Section Title: {title}

Content:
{truncated_text}

Provide a concise summary that captures the main points."""
        
        try:
            summary = await llm.chat(prompt)
            node['summary'] = summary.strip() if summary else ""
        except Exception as e:
            print(f"Error generating summary for '{title}': {e}")
            node['summary'] = ""
    
    async def process_node_recursive(node):
        """é€’å½’å¤„ç†èŠ‚ç‚¹"""
        # ä¸ºå½“å‰èŠ‚ç‚¹ç”Ÿæˆæ‘˜è¦
        await generate_summary(node)
        
        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'nodes' in node and node['nodes']:
            tasks = [process_node_recursive(child) for child in node['nodes']]
            await asyncio.gather(*tasks)
    
    # å¹¶å‘å¤„ç†æ‰€æœ‰æ ¹èŠ‚ç‚¹
    tasks = [process_node_recursive(root) for root in structure]
    await asyncio.gather(*tasks)


def _remove_node_text(structure: list):
    """
    é€’å½’ç§»é™¤èŠ‚ç‚¹çš„ text å­—æ®µ
    
    Args:
        structure: æ ‘ç»“æ„
    """
    for node in structure:
        if 'text' in node:
            del node['text']
        
        if 'nodes' in node and node['nodes']:
            _remove_node_text(node['nodes'])


# ============================================================================
# å¯¼å‡ºæ¥å£ï¼ˆä¸è€ç‰ˆæœ¬ pageindex å®Œå…¨ä¸€è‡´ï¼‰
# ============================================================================

__all__ = [
    'page_index_main',
    'config',
    'ConfigLoader',
]
