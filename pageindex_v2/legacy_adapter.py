"""
Legacy Adapter for PageIndex V2
å…¼å®¹è€ pageindex API çš„é€‚é…å±‚

æä¾›ä¸è€ç‰ˆæœ¬ page_index_main å®Œå…¨ç›¸åŒçš„æ¥å£ï¼Œç¡®ä¿ä¸‹æ¸¸ç³»ç»Ÿæ— éœ€ä¿®æ”¹å³å¯ä½¿ç”¨æ–°ç®—æ³•ã€‚

Usage:
    from pageindex_v2.legacy_adapter import page_index_main, config, ConfigLoader
    
    opt = ConfigLoader().load({"model": "gpt-4o-2024-11-20"})
    result = page_index_main("path/to/file.pdf", opt)
"""

import os
import asyncio
import time
from io import BytesIO
from typing import Dict, Any, Optional, Union
from types import SimpleNamespace


def config(**kwargs):
    """
    å…¼å®¹è€ç‰ˆæœ¬çš„ config å¯¹è±¡ï¼ˆSimpleNamespaceï¼‰
    """
    return SimpleNamespace(**kwargs)


class ConfigLoader:
    """
    å…¼å®¹è€ç‰ˆæœ¬çš„ ConfigLoader
    åŠ è½½é»˜è®¤é…ç½®å¹¶ä¸ç”¨æˆ·é…ç½®åˆå¹¶
    """
    
    DEFAULT_CONFIG = {
        "model": "gpt-4o-2024-11-20",
        "toc_check_page_num": 20,
        "max_page_num_each_node": 10,
        "max_token_num_each_node": 20000,
        "if_add_node_id": "yes",
        "if_add_node_summary": "no",
        "if_add_doc_description": "no",
        "if_add_node_text": "no",
        "custom_prompt": None
    }
    
    def load(self, user_opt: Optional[Union[Dict, SimpleNamespace]] = None) -> SimpleNamespace:
        """
        åŠ è½½é…ç½®ï¼Œåˆå¹¶ç”¨æˆ·é€‰é¡¹ä¸é»˜è®¤å€¼
        
        Args:
            user_opt: ç”¨æˆ·é…ç½®ï¼ˆdict æˆ– SimpleNamespaceï¼‰
            
        Returns:
            SimpleNamespace é…ç½®å¯¹è±¡
        """
        if user_opt is None:
            user_dict = {}
        elif isinstance(user_opt, SimpleNamespace):
            user_dict = vars(user_opt)
        elif isinstance(user_opt, dict):
            user_dict = user_opt
        else:
            raise TypeError("user_opt must be dict, SimpleNamespace or None")
        
        # åˆå¹¶é…ç½®
        merged = {**self.DEFAULT_CONFIG, **user_dict}
        return config(**merged)


def page_index_main(doc: Union[str, BytesIO], opt: Optional[SimpleNamespace] = None) -> Dict[str, Any]:
    """
    ä¸»å…¥å£å‡½æ•° - å…¼å®¹è€ç‰ˆæœ¬ pageindex.page_index_main API
    
    Args:
        doc: PDFæ–‡ä»¶è·¯å¾„ï¼ˆstrï¼‰æˆ– BytesIO å¯¹è±¡
        opt: é…ç½®å¯¹è±¡ï¼ˆç”± ConfigLoader ç”Ÿæˆï¼‰
        
    Returns:
        å…¼å®¹è€æ ¼å¼çš„è¾“å‡ºï¼š
        {
          "result": {
            "doc_name": "xxx.pdf",
            "structure": [...]
          },
          "performance": {...}
        }
    """
    # åŠ è½½é»˜è®¤é…ç½®
    if opt is None:
        opt = ConfigLoader().load()
    
    # è½¬æ¢é…ç½®åˆ° ProcessingOptions
    options = _convert_old_opt_to_v2(opt)
    
    # å¤„ç† BytesIO è¾“å…¥
    pdf_path, temp_file = _prepare_pdf_input(doc)
    
    try:
        # å¯¼å…¥æ–°ç®—æ³•ï¼ˆä½¿ç”¨ç›¸å¯¹å¯¼å…¥ï¼‰
        from .main import PageIndexV2
        
        # è®¾ç½® document_idï¼ˆç”¨äº progress callbackï¼‰
        doc_id = _setup_progress_callback()
        
        # è°ƒç”¨æ–°ç®—æ³•
        start_time = time.time()
        processor = PageIndexV2(options, document_id=doc_id)
        
        # åŒ…è£…è¿›åº¦æŠ¥å‘Š
        _wrap_progress_reporting(processor)
        
        # æ‰§è¡Œå¤„ç†ï¼ˆå¼‚æ­¥è½¬åŒæ­¥ï¼‰
        v2_result = asyncio.run(processor.process_pdf(pdf_path))
        
        total_time = time.time() - start_time
        
        # è½¬æ¢è¾“å‡ºæ ¼å¼
        old_format = _convert_v2_to_old_format(v2_result, opt, total_time)
        
        # åå¤„ç†ï¼šæ·»åŠ  node_id, text, summaryï¼ˆæ ¹æ®é…ç½®ï¼‰
        structure = old_format["result"]["structure"]
        
        if getattr(opt, 'if_add_node_id', 'yes') == 'yes':
            _add_node_ids(structure)
        
        if getattr(opt, 'if_add_node_text', 'no') == 'yes':
            _add_node_text(structure, pdf_path)
        
        if getattr(opt, 'if_add_node_summary', 'no') == 'yes':
            # Summary éœ€è¦ textï¼Œå¦‚æœä¹‹å‰æ²¡åŠ ï¼Œä¸´æ—¶åŠ ä¸Š
            needs_temp_text = getattr(opt, 'if_add_node_text', 'no') == 'no'
            if needs_temp_text:
                _add_node_text(structure, pdf_path)
            
            # ç”Ÿæˆ summariesï¼ˆå¼‚æ­¥ï¼‰
            asyncio.run(_add_node_summaries(structure, getattr(opt, 'model', 'gpt-4o-2024-11-20')))
            
            # ç§»é™¤ä¸´æ—¶ text
            if needs_temp_text:
                _remove_node_text(structure)
        
        return old_format
        
    finally:
        # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
        if temp_file:
            try:
                os.unlink(temp_file)
            except:
                pass


def _convert_old_opt_to_v2(opt: SimpleNamespace):
    """
    å°†è€é…ç½®è½¬æ¢ä¸ºæ–°çš„ ProcessingOptions
    
    æ˜ å°„å…³ç³»ï¼š
    - model: ç›´æ¥æ˜ å°„
    - toc_check_page_num -> toc_check_pages
    - max_page_num_each_node -> max_pages_per_node
    - max_token_num_each_node -> max_tokens_per_node
    """
    from .main import ProcessingOptions
    
    # ç¡®å®š providerï¼ˆæ ¹æ® model æ¨æ–­æˆ–ä»ç¯å¢ƒå˜é‡è¯»å–ï¼‰
    import os
    model = getattr(opt, 'model', 'gpt-4o-2024-11-20')
    model_lower = model.lower()

    # æ ¹æ® model åç§°è¯†åˆ« provider
    if 'deepseek' in model_lower:
        provider = 'deepseek'
    elif 'openrouter' in model_lower or any(x in model_lower for x in ['arcee-ai', 'mimo', 'xiaomi']):
        provider = 'openrouter'
    elif 'gemini' in model_lower:
        provider = 'gemini'
    elif 'glm' in model_lower or 'zhipu' in model_lower:
        provider = 'zhipu'
    else:
        # é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
        provider = os.getenv('LLM_PROVIDER', 'deepseek')
    
    return ProcessingOptions(
        provider=provider,
        model=model,
        max_depth=4,  # æ–°ç®—æ³•å›ºå®šä¸º4å±‚
        toc_check_pages=getattr(opt, 'toc_check_page_num', 20),
        debug=getattr(opt, 'debug', False),  # ç”±è°ƒç”¨æ–¹æ§åˆ¶è°ƒè¯•è¾“å‡º
        progress=True,  # ä¿æŒè¿›åº¦è¾“å‡º
        output_dir="./results",
        enable_recursive_processing=True,
        skip_verification_for_large_pdf=True,
        large_pdf_threshold=200,
        max_pages_per_node=getattr(opt, 'max_page_num_each_node', 10),
        max_tokens_per_node=getattr(opt, 'max_token_num_each_node', 20000),
        max_verify_count=100,
        verification_concurrency=20
    )


def _prepare_pdf_input(doc: Union[str, BytesIO]) -> tuple:
    """
    å¤„ç† PDF è¾“å…¥ï¼Œæ”¯æŒæ–‡ä»¶è·¯å¾„å’Œ BytesIO
    
    Returns:
        (pdf_path, temp_file_path)
        - pdf_path: å®é™…çš„æ–‡ä»¶è·¯å¾„
        - temp_file_path: å¦‚æœæ˜¯ BytesIOï¼Œè¿”å›ä¸´æ—¶æ–‡ä»¶è·¯å¾„ï¼›å¦åˆ™è¿”å› None
    """
    if isinstance(doc, BytesIO):
        # BytesIO éœ€è¦ä¿å­˜åˆ°ä¸´æ—¶æ–‡ä»¶
        import tempfile
        
        with tempfile.NamedTemporaryFile(delete=False, suffix='.pdf', mode='wb') as tmp:
            tmp.write(doc.getvalue())
            temp_path = tmp.name
        
        return temp_path, temp_path
    
    elif isinstance(doc, str):
        # å­—ç¬¦ä¸²è·¯å¾„
        if not os.path.isfile(doc):
            raise FileNotFoundError(f"PDF file not found: {doc}")
        return doc, None
    
    else:
        raise TypeError(f"Unsupported input type: {type(doc)}. Expected str or BytesIO.")


def _setup_progress_callback():
    """
    è®¾ç½®è¿›åº¦å›è°ƒï¼ˆå…¼å®¹è€ç³»ç»Ÿçš„ progress_callback æ¨¡å—ï¼‰
    """
    try:
        from pageindex.progress_callback import get_document_id
        doc_id = get_document_id()
        # document_id å·²é€šè¿‡ pageindex.progress_callback è®¾ç½®
        return doc_id
    except ImportError:
        # progress_callback æ¨¡å—ä¸å¯ç”¨ï¼Œè·³è¿‡
        return None


def _wrap_progress_reporting(processor):
    """
    åŒ…è£… PageIndexV2 çš„è¿›åº¦æŠ¥å‘Šï¼Œè½¬å‘åˆ°è€ç³»ç»Ÿçš„ progress_callback
    
    Args:
        processor: PageIndexV2 å®ä¾‹
    """
    try:
        from pageindex.progress_callback import report_progress, get_document_id
        
        doc_id = get_document_id()
        if not doc_id:
            return  # æ²¡æœ‰ document_idï¼Œè·³è¿‡
        
        # ä¿å­˜åŸå§‹å‡½æ•°
        original_log_progress = processor.log_progress
        
        def wrapped_log_progress(message: str, force: bool = False):
            """åŒ…è£…åçš„è¿›åº¦æŠ¥å‘Šå‡½æ•°"""
            # è°ƒç”¨åŸå§‹å‡½æ•°
            original_log_progress(message, force)
            
            # è§£ææ¶ˆæ¯æå–é˜¶æ®µå’Œè¿›åº¦
            stage, progress_pct, clean_msg = _parse_progress_message(message)
            
            if stage and progress_pct is not None:
                # è½¬å‘åˆ°è€ç³»ç»Ÿ
                report_progress(doc_id, stage, progress=progress_pct, message=clean_msg)
        
        # æ›¿æ¢å‡½æ•°
        processor.log_progress = wrapped_log_progress
        
    except ImportError:
        # progress_callback ä¸å¯ç”¨ï¼Œè·³è¿‡
        pass


def _parse_progress_message(message: str) -> tuple:
    """
    è§£æè¿›åº¦æ¶ˆæ¯ï¼Œæå–é˜¶æ®µåå’Œè¿›åº¦ç™¾åˆ†æ¯”
    
    Args:
        message: å¦‚ "ğŸ“„ [1/6] PDF Parsing... (30 pages)"
        
    Returns:
        (stage, progress_percent, clean_message)
    """
    import re
    
    # é»˜è®¤å€¼
    stage = "processing"
    progress_pct = None
    clean_msg = message
    
    # åŒ¹é…æ¨¡å¼: [X/Y]
    match = re.search(r'\[(\d+)/(\d+)\]', message)
    if match:
        current = int(match.group(1))
        total = int(match.group(2))
        # æ˜ å°„åˆ° 0-100 çš„è¿›åº¦
        progress_pct = (current - 1) / total * 100
        
        # æå–é˜¶æ®µå
        stage_match = re.search(r'\[(\d+)/\d+\]\s*([^.]+)', message)
        if stage_match:
            stage_name = stage_match.group(2).strip()
            # æ˜ å°„åˆ°è€ç³»ç»Ÿçš„é˜¶æ®µå
            stage_mapping = {
                'PDF Parsing': 'pdf_parsing',
                'TOC Detection': 'toc_detection',
                'Structure Extraction': 'toc_transformation',
                'Page Mapping': 'page_mapping',
                'Verification': 'toc_verification',
                'Tree Building': 'tree_building'
            }
            stage = stage_mapping.get(stage_name, 'processing')
    
    return stage, progress_pct, clean_msg


def _convert_v2_to_old_format(v2_result: Dict, opt: SimpleNamespace, total_time: float) -> Dict[str, Any]:
    """
    è½¬æ¢æ–°ç®—æ³•è¾“å‡ºä¸ºè€æ ¼å¼
    
    Args:
        v2_result: æ–°ç®—æ³•çš„è¾“å‡º
        opt: é…ç½®å¯¹è±¡
        total_time: æ€»å¤„ç†æ—¶é—´
        
    Returns:
        è€æ ¼å¼è¾“å‡ºï¼š
        {
          "result": {
            "doc_name": "...",
            "structure": [...]
          },
          "performance": {...}
        }
    """
    # æå–åŸºæœ¬ä¿¡æ¯
    doc_name = v2_result.get("source_file", "unknown.pdf")
    structure = v2_result.get("structure", [])
    stats = v2_result.get("statistics", {})
    
    # æ ¹æ®é…ç½®æ·»åŠ  doc_descriptionï¼ˆå¦‚æœéœ€è¦ï¼‰
    result_dict = {
        "doc_name": doc_name,
        "structure": structure
    }
    
    if getattr(opt, 'if_add_doc_description', 'no') == 'yes' and v2_result.get("doc_description"):
        result_dict["doc_description"] = v2_result.get("doc_description")
    
    # æ„é€ æ€§èƒ½æ•°æ®ï¼ˆå…¼å®¹è€æ ¼å¼ï¼‰
    performance = {
        "total_time": total_time,
        "tree_building": {
            "duration": total_time * 0.7,  # ä¼°ç®—ï¼šæ ‘æ„å»ºå 70%
            "items_processed": stats.get("total_nodes", 0)
        },
        "toc_detection": {
            "duration": total_time * 0.1,  # ä¼°ç®—ï¼šTOCæ£€æµ‹å 10%
        },
        "toc_transformation": {
            "duration": total_time * 0.1,  # ä¼°ç®—ï¼šè½¬æ¢å 10%
        },
        "verification": {
            "duration": total_time * 0.1,  # ä¼°ç®—ï¼šéªŒè¯å 10%
            "accuracy": v2_result.get("verification_accuracy", 1.0)
        },
        "summary": {
            "total_nodes": stats.get("total_nodes", 0),
            "max_depth": stats.get("max_depth", 0),
            "root_nodes": stats.get("root_nodes", 0),
            "mapping_accuracy": v2_result.get("mapping_validation_accuracy", 1.0),
            "verification_accuracy": v2_result.get("verification_accuracy", 1.0)
        }
    }
    
    # è¿”å›å…¼å®¹æ ¼å¼
    return {
        "result": result_dict,
        "performance": performance
    }


# ============================================================================
# è¾…åŠ©å‡½æ•°ï¼šæ·»åŠ  node_id, text, summary
# ============================================================================

def _add_node_ids(structure: list, node_id: int = 0, use_hierarchical: bool = True):
    """
    é€’å½’æ·»åŠ  node_id

    Args:
        structure: æ ‘ç»“æ„ï¼ˆåˆ—è¡¨ï¼‰
        node_id: å½“å‰èŠ‚ç‚¹IDè®¡æ•°å™¨ï¼ˆä»…ç”¨äºé¡ºåºæ¨¡å¼ï¼‰
        use_hierarchical: æ˜¯å¦ä½¿ç”¨å¤šçº§ç¼–å· (1, 1.1, 1.1.1)
                          False åˆ™ä½¿ç”¨é¡ºåºç¼–å· (0000, 0001, 0002)
    """
    if use_hierarchical:
        # Use the hierarchical numbering from helpers.py
        from .utils.helpers import add_node_ids
        add_node_ids(structure, use_hierarchical=True)
        # Return a dummy value (not used in hierarchical mode)
        return 0
    else:
        # Original sequential numbering
        for item in structure:
            item['node_id'] = str(node_id).zfill(4)
            node_id += 1

            if 'nodes' in item and item['nodes']:
                node_id = _add_node_ids(item['nodes'], node_id, use_hierarchical=False)

        return node_id


def _find_title_in_text(text: str, title: str) -> int:
    """
    åœ¨é¡µé¢æ–‡æœ¬ä¸­æŸ¥æ‰¾æ ‡é¢˜ä½ç½®ï¼Œæ”¯æŒå¤šç§åŒ¹é…ç­–ç•¥ã€‚

    ç­–ç•¥ä¼˜å…ˆçº§ï¼š
    1. ç²¾ç¡®åŒ¹é…
    2. å»é™¤ç©ºç™½å·®å¼‚ååŒ¹é…
    3. å»é™¤ç¼–å·å‰ç¼€ååŒ¹é…æ ¸å¿ƒå†…å®¹
    4. æ ‡é¢˜å‰ç¼€åŒ¹é…ï¼ˆå‰8ä¸ªæœ‰æ•ˆå­—ç¬¦ï¼‰

    Args:
        text: é¡µé¢æ–‡æœ¬
        title: èŠ‚ç‚¹æ ‡é¢˜

    Returns:
        æ ‡é¢˜åœ¨æ–‡æœ¬ä¸­çš„èµ·å§‹ä½ç½®ï¼Œæœªæ‰¾åˆ°è¿”å› -1
    """
    import re

    if not text or not title:
        return -1

    title = title.strip()

    # ç­–ç•¥1ï¼šç²¾ç¡®åŒ¹é…
    pos = text.find(title)
    if pos >= 0:
        return pos

    # ç­–ç•¥2ï¼šè§„èŒƒåŒ–ç©ºç™½ååŒ¹é…ï¼ˆå°†è¿ç»­ç©ºç™½æ›¿æ¢ä¸ºå•ç©ºæ ¼ï¼‰
    normalized_title = re.sub(r'\s+', ' ', title).strip()
    if normalized_title != title:
        pos = text.find(normalized_title)
        if pos >= 0:
            return pos

    # ç­–ç•¥3ï¼šå»é™¤å¸¸è§ç¼–å·å‰ç¼€ååŒ¹é…æ ¸å¿ƒå†…å®¹
    # åŒ¹é…ï¼šï¼ˆä¸€ï¼‰ã€ï¼ˆäºŒï¼‰ã€(1)ã€(2)ã€1.ã€1ã€ç¬¬Xç« ã€ç¬¬XèŠ‚ ç­‰
    core_patterns = [
        r'^[ï¼ˆ(]\s*[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶\d]+\s*[ï¼‰)]\s*',  # ï¼ˆä¸€ï¼‰ã€(1)
        r'^ç¬¬[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹åç™¾é›¶\d]+[ç« èŠ‚éƒ¨åˆ†æ¡æ¬¾]\s*',    # ç¬¬ä¸€ç« ã€ç¬¬1èŠ‚
        r'^[ä¸€äºŒä¸‰å››äº”å…­ä¸ƒå…«ä¹å]+\s*[ã€ï¼.]\s*',              # ä¸€ã€äºŒã€
        r'^\d+\s*[ã€ï¼.]\s*',                                 # 1ã€2.
        r'^\d+(\.\d+)*\s+',                                   # 1.1 ã€1.1.1
        r'^[A-Z]\s*[ã€ï¼.]\s*',                                # Aã€B.
    ]

    for pattern in core_patterns:
        core = re.sub(pattern, '', title).strip()
        if core and len(core) >= 2 and core != title:
            pos = text.find(core)
            if pos >= 0:
                # å›é€€æŸ¥æ‰¾ï¼šåœ¨ core ä¹‹å‰å¯èƒ½æœ‰ç¼–å·å‰ç¼€ï¼Œå‘å‰æœç´¢ä¸€è¡Œ
                line_start = text.rfind('\n', max(0, pos - 50), pos)
                if line_start >= 0:
                    return line_start + 1  # +1 è·³è¿‡ \n
                return max(0, pos - 30)  # ç²—ç•¥å›é€€åˆ°å¯èƒ½çš„è¡Œé¦–

    # ç­–ç•¥4ï¼šæ ‡é¢˜å‰ç¼€åŒ¹é…ï¼ˆå–å‰8ä¸ªæœ‰æ•ˆå­—ç¬¦ï¼‰
    if len(title) > 4:
        prefix = title[:min(8, len(title))]
        pos = text.find(prefix)
        if pos >= 0:
            return pos

    return -1


def _extract_section_text(
    node_title: str,
    next_sibling_title: str,
    start_page: int,
    end_page: int,
    get_page_fn,
    max_chars: int = 2000
) -> str:
    """
    æå–èŠ‚ç‚¹çš„æ®µè½çº§æ–‡æœ¬ï¼Œåœ¨å…±äº«é¡µé¢ä¸Šæ ¹æ®æ ‡é¢˜ä½ç½®åˆ‡åˆ†ã€‚

    Args:
        node_title: å½“å‰èŠ‚ç‚¹æ ‡é¢˜
        next_sibling_title: ä¸‹ä¸€ä¸ªå…„å¼ŸèŠ‚ç‚¹æ ‡é¢˜ï¼ˆç”¨äºç¡®å®šç»“æŸè¾¹ç•Œï¼‰ï¼Œå¯ä¸º None
        start_page: èµ·å§‹é¡µç ï¼ˆ1-indexedï¼‰
        end_page: ç»“æŸé¡µç ï¼ˆ1-indexedï¼‰
        get_page_fn: è·å–é¡µé¢æ–‡æœ¬çš„å‡½æ•° (page_num) -> str
        max_chars: æœ€å¤§å­—ç¬¦æ•°é™åˆ¶

    Returns:
        æå–çš„æ–‡æœ¬å†…å®¹
    """
    text_parts = []

    for page_num in range(start_page, end_page + 1):
        page_text = get_page_fn(page_num)
        if not page_text:
            continue

        # åœ¨èµ·å§‹é¡µï¼šä»å½“å‰æ ‡é¢˜ä½ç½®å¼€å§‹
        if page_num == start_page and node_title:
            title_pos = _find_title_in_text(page_text, node_title)
            if title_pos > 0:
                page_text = page_text[title_pos:]

        # åœ¨ç»“æŸé¡µï¼šåœ¨ä¸‹ä¸€ä¸ªå…„å¼Ÿæ ‡é¢˜ä½ç½®æˆªæ–­
        if page_num == end_page and next_sibling_title:
            next_pos = _find_title_in_text(page_text, next_sibling_title)
            if next_pos > 0:
                page_text = page_text[:next_pos]

        stripped = page_text.strip()
        if stripped:
            text_parts.append(stripped)

    full_text = "\n".join(text_parts)

    # æˆªæ–­åˆ° max_chars
    if len(full_text) > max_chars:
        full_text = full_text[:max_chars]

    return full_text


def _add_node_text(structure: list, pdf_path: str):
    """
    æ·»åŠ èŠ‚ç‚¹æ–‡æœ¬å†…å®¹ï¼ˆæ®µè½çº§åˆ‡åˆ†ç‰ˆæœ¬ï¼‰

    æ”¹è¿›ç­–ç•¥ï¼š
    - åŒé¡µå¤šèŠ‚ç‚¹ï¼šæ ¹æ®æ ‡é¢˜ä½ç½®åœ¨é¡µé¢æ–‡æœ¬å†…åˆ‡åˆ†ï¼Œæ¯ä¸ªèŠ‚ç‚¹åªæ‹¿åˆ°è‡ªå·±çš„æ®µè½
    - è·¨é¡µèŠ‚ç‚¹ï¼šèµ·å§‹é¡µä»æ ‡é¢˜å¤„å¼€å§‹ï¼Œç»“æŸé¡µåœ¨ä¸‹ä¸€èŠ‚æ ‡é¢˜å¤„æˆªæ–­
    - çˆ¶èŠ‚ç‚¹ï¼šä¸æ·»åŠ å†…å®¹ï¼ˆä½¿ç”¨ summary æ›¿ä»£ï¼‰
    - æ–‡æœ¬ä¸Šé™æå‡åˆ° 2000 å­—ç¬¦

    Args:
        structure: æ ‘ç»“æ„
        pdf_path: PDFæ–‡ä»¶è·¯å¾„
    """
    import fitz  # PyMuPDF

    doc = fitz.open(pdf_path)

    # ç¼“å­˜é¡µé¢æ–‡æœ¬ï¼Œé¿å…é‡å¤æå–
    page_cache = {}

    def get_page_text(page_num: int) -> str:
        """è·å–æŒ‡å®šé¡µç çš„æ–‡æœ¬ï¼ˆå¸¦ç¼“å­˜ï¼Œ1-indexedï¼‰"""
        if page_num not in page_cache:
            idx = page_num - 1
            if 0 <= idx < len(doc):
                page_cache[page_num] = doc[idx].get_text()
            else:
                page_cache[page_num] = ""
        return page_cache[page_num]

    def process_siblings(siblings: list):
        """å¤„ç†ä¸€ç»„å…„å¼ŸèŠ‚ç‚¹ï¼Œåˆ©ç”¨å…„å¼Ÿä¿¡æ¯åšæ®µè½çº§åˆ‡åˆ†"""
        for i, node in enumerate(siblings):
            has_children = 'nodes' in node and node['nodes']

            if has_children:
                # çˆ¶èŠ‚ç‚¹ï¼šç©ºæ–‡æœ¬ï¼Œé€’å½’å¤„ç†å­èŠ‚ç‚¹
                node['text'] = ""
                process_siblings(node['nodes'])
            else:
                # å¶å­èŠ‚ç‚¹ï¼šæå–æ®µè½çº§æ–‡æœ¬
                start = node.get('start_index', 1)
                end = node.get('end_index', start)

                # ç¡®å®šä¸‹ä¸€ä¸ªå…„å¼ŸèŠ‚ç‚¹çš„æ ‡é¢˜ï¼ˆç”¨äºç»“æŸè¾¹ç•Œæ£€æµ‹ï¼‰
                next_sibling_title = None
                if i + 1 < len(siblings):
                    next_sib = siblings[i + 1]
                    next_sib_start = next_sib.get('start_index', end + 1)
                    # åªæœ‰å½“ä¸‹ä¸€ä¸ªå…„å¼Ÿçš„èµ·å§‹é¡µåœ¨å½“å‰èŠ‚ç‚¹çš„ç»“æŸé¡µèŒƒå›´å†…æ—¶æ‰éœ€è¦åˆ‡åˆ†
                    if next_sib_start <= end:
                        next_sibling_title = next_sib.get('title', '')

                node['text'] = _extract_section_text(
                    node_title=node.get('title', ''),
                    next_sibling_title=next_sibling_title,
                    start_page=start,
                    end_page=end,
                    get_page_fn=get_page_text,
                    max_chars=2000
                )

    # æ ¹èŠ‚ç‚¹æœ¬èº«å°±æ˜¯ä¸€ç»„å…„å¼Ÿ
    process_siblings(structure)

    doc.close()


async def _add_node_summaries(structure: list, model: str):
    """
    å¼‚æ­¥ç”ŸæˆèŠ‚ç‚¹æ‘˜è¦
    
    Args:
        structure: æ ‘ç»“æ„
        model: LLMæ¨¡å‹åç§°
    """
    from .core.llm_client import LLMClient
    
    # åˆå§‹åŒ– LLM client - æ ¹æ® model åç§°è¯†åˆ« provider
    model_lower = model.lower()
    if 'deepseek' in model_lower:
        provider = 'deepseek'
    elif 'openrouter' in model_lower or any(x in model_lower for x in ['arcee-ai', 'mimo', 'xiaomi']):
        provider = 'openrouter'
    elif 'gemini' in model_lower:
        provider = 'gemini'
    elif 'glm' in model_lower or 'zhipu' in model_lower:
        provider = 'zhipu'
    else:
        # é»˜è®¤ä»ç¯å¢ƒå˜é‡è¯»å–
        import os
        provider = os.getenv('LLM_PROVIDER', 'deepseek')

    llm = LLMClient(provider=provider, model=model, debug=False)
    
    def _detect_language(text: str) -> str:
        """æ£€æµ‹æ–‡æœ¬ä¸»è¦è¯­è¨€ï¼š'zh' æˆ– 'en'"""
        import re
        chinese_chars = len(re.findall(r'[\u4e00-\u9fff]', text[:500]))
        total_alpha = len(re.findall(r'[a-zA-Z]', text[:500]))
        return 'zh' if chinese_chars > total_alpha else 'en'

    # æ£€æµ‹æ–‡æ¡£è¯­è¨€ï¼ˆå–ç¬¬ä¸€ä¸ªæœ‰æ–‡æœ¬çš„å¶å­èŠ‚ç‚¹ï¼‰
    doc_lang = 'en'
    def _find_first_text(nodes):
        for n in nodes:
            t = n.get('text', '')
            if t and len(t.strip()) > 20:
                return t
            children = n.get('nodes', [])
            if children:
                result = _find_first_text(children)
                if result:
                    return result
        return None

    sample_text = _find_first_text(structure)
    if sample_text:
        doc_lang = _detect_language(sample_text)

    async def generate_summary(node):
        """ä¸ºå•ä¸ªèŠ‚ç‚¹ç”Ÿæˆæ‘˜è¦"""
        text = node.get('text', '')
        title = node.get('title', '')

        if not text or len(text.strip()) < 10:
            node['summary'] = ""
            return

        truncated_text = text[:3000]

        if doc_lang == 'zh':
            prompt = f"""è¯·ç”¨1-2å¥ä¸­æ–‡æ¦‚æ‹¬ä»¥ä¸‹ç« èŠ‚çš„æ ¸å¿ƒå†…å®¹ã€‚

ç« èŠ‚æ ‡é¢˜ï¼š{title}

å†…å®¹ï¼š
{truncated_text}

è¯·ç›´æ¥ç»™å‡ºæ‘˜è¦ï¼Œä¸è¦æ·»åŠ å‰ç¼€ã€‚"""
        else:
            prompt = f"""Summarize the following section in 1-2 sentences.

Section Title: {title}

Content:
{truncated_text}

Provide a concise summary that captures the main points."""

        try:
            summary = await llm.chat(prompt)
            node['summary'] = summary.strip() if summary else ""
        except Exception as e:
            print(f"Error generating summary for '{title}': {e}")
            node['summary'] = ""
    
    async def process_node_recursive(node):
        """é€’å½’å¤„ç†èŠ‚ç‚¹"""
        # ä¸ºå½“å‰èŠ‚ç‚¹ç”Ÿæˆæ‘˜è¦
        await generate_summary(node)
        
        # é€’å½’å¤„ç†å­èŠ‚ç‚¹
        if 'nodes' in node and node['nodes']:
            tasks = [process_node_recursive(child) for child in node['nodes']]
            await asyncio.gather(*tasks)
    
    # å¹¶å‘å¤„ç†æ‰€æœ‰æ ¹èŠ‚ç‚¹
    tasks = [process_node_recursive(root) for root in structure]
    await asyncio.gather(*tasks)
    
    # Clean up LLM client
    await llm.close()


def _remove_node_text(structure: list):
    """
    é€’å½’ç§»é™¤èŠ‚ç‚¹çš„ text å­—æ®µ
    
    Args:
        structure: æ ‘ç»“æ„
    """
    for node in structure:
        if 'text' in node:
            del node['text']
        
        if 'nodes' in node and node['nodes']:
            _remove_node_text(node['nodes'])


# ============================================================================
# å¯¼å‡ºæ¥å£ï¼ˆä¸è€ç‰ˆæœ¬ pageindex å®Œå…¨ä¸€è‡´ï¼‰
# ============================================================================

__all__ = [
    'page_index_main',
    'config',
    'ConfigLoader',
]
